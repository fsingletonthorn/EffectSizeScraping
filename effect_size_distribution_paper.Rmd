---
title: "pilot_paper"
author: "Felix Singleton Thorn"
date: "16/09/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

One of the most useful tools that facilitates succinct and understandable data summaries, comparison of results across studies and future power analysis are standardised effect sizes. Although statistical significance tests are often presented as the primary tests of a scientific theory, equally if not more important is the degree of difference or strength of effect that was observed. Often, the most efficient way of expressing the size of the effect will be to use a standardised effect size (e.g., Cohen’s d, partial \eta^2, R2). These effect size benchmarks have become extremely tightly engrained in much of the discussion around effect sizes, especially in psychology. Many of the most commonly used benchmarks to understand and planning effect sizes in psychology are based on Cohen’s (1988) benchmarks. This is true despite the fact that Cohen never suggested using his benchmarks as anything but a last resort in power analysis or effect size interpretation. 

E.g.; “When the above has not provided sufficient guidance, the reader has an additional recourse. For each statistical test's ES index, the author proposes, as a convention, ES values to serve as operational definitions of the qualitative adjectives" small,"" medium," and" large." This is an operation fraught with many dangers … they run a risk of being misunderstood.”, Cohen, 1988, p.12. 

And despite the fact that he selected these benchmarks in a somewhat ad hoc manner e.g., “‘Small’ effect sizes must not be so small that seeking them amidst the inevitable operation of measurement and experimental bias and lack of fidelity is a bootless task, yet not so large as to make them fairly perceptible to the naked observational eye… In contrast, large effects must not be defined as so large that their quest by statistical methods is wholly a labor of supererogation.” Cohen, 1988, p.13. 

This project uses a big data and text mining approach to extract effect sizes reported in standard formats in over in over two million open access articles in the PubMedCentral open access subset, and presents an online tool for accessing, visualising and downloading this dataset. This does not provide a method for estimating the expected outcome of a particular area of psychological research, just of visualising the reported effects in an area of psychology research. It can be used as heuristic guide to understanding effect sizes, but for specific estimates of the expected effect sizes from particular experiments, researchers should look to meta-analyses or other large scale replication curation projects like curatescience.org (LeBel, McCarthy, Earp, Elson, & Vanpaemel, 2018). 
