<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Psychol</journal-id><journal-id journal-id-type="iso-abbrev">Front Psychol</journal-id><journal-id journal-id-type="publisher-id">Front. Psychol.</journal-id><journal-title-group><journal-title>Frontiers in Psychology</journal-title></journal-title-group><issn pub-type="epub">1664-1078</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">27920751</article-id><article-id pub-id-type="pmc">5118620</article-id><article-id pub-id-type="doi">10.3389/fpsyg.2016.01847</article-id><article-categories><subj-group subj-group-type="heading"><subject>Psychology</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>The Temporal Dynamics of Perceiving Other&#x02019;s Painful Actions</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Cui</surname><given-names>Fang</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="author-notes" rid="fn001"><sup>*</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/233616/overview"/></contrib><contrib contrib-type="author"><name><surname>Gu</surname><given-names>Ruolei</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/70325/overview"/></contrib><contrib contrib-type="author"><name><surname>Zhu</surname><given-names>Xiangru</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/258682/overview"/></contrib><contrib contrib-type="author"><name><surname>Luo</surname><given-names>Yue-jia</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="aff" rid="aff5"><sup>5</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/68985/overview"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Institute of Affective and Social Neuroscience, Shenzhen University, Shenzhen</institution><country>China</country></aff><aff id="aff2"><sup>2</sup><institution>CAS Key Laboratory of Behavioral Science, Institute of Psychology, Beijing</institution><country>China</country></aff><aff id="aff3"><sup>3</sup><institution>Institute of Cognition and Behavior, Henan University, Kaifeng</institution><country>China</country></aff><aff id="aff4"><sup>4</sup><institution>Department of Psychology, Southern Medical University, Guangzhou</institution><country>China</country></aff><aff id="aff5"><sup>5</sup><institution>Shenzhen Institute of Neuroscience, Shenzhen</institution><country>China</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: <italic>Seung-Lark Lim, University of Missouri&#x02013;Kansas City, USA</italic></p></fn><fn fn-type="edited-by"><p>Reviewed by: <italic>Giovanni Mirabella, Sapienza University of Rome, Italy; Tae-Ho Lee, University of North Carolina at Chapel Hill, USA</italic></p></fn><corresp id="fn001">*Correspondence: <italic>Fang Cui, <email xlink:type="simple">cuifang0826@gmail.com</email></italic></corresp><fn fn-type="other" id="fn002"><p>This article was submitted to Emotion Science, a section of the journal Frontiers in Psychology</p></fn></author-notes><pub-date pub-type="epub"><day>22</day><month>11</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>7</volume><elocation-id>1847</elocation-id><history><date date-type="received"><day>20</day><month>7</month><year>2016</year></date><date date-type="accepted"><day>08</day><month>11</month><year>2016</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2016 Cui, Gu, Zhu and Luo.</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Cui, Gu, Zhu and Luo</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>The present study investigates the temporal dynamics of the brain activity predicting the sensory outcomes of observed hand&#x02013;object interactions of others. Participants are presented with pictures of a hand grasping or withdrawing from noxious and neutral objects. They are then asked to judge whether this hand&#x02013;object interaction causes painful consequences. In the early stages of stimulus processing, the effect of action was observed in the event-related potential components N1 and N2. Significant interactions of action &#x000d7; object were observed in the later components P3 and late positive potential (LPP): only when the object was noxious, the action &#x0201c;grasp&#x0201d; elicited a significantly larger amplitude than the action &#x0201c;withdrawal&#x0201d;. These results suggest that: on the one hand, when observing the hand&#x02013;object interaction from the third-person perspective, the action type of others can be processed in an automatic style. On the other hand, integrating the information of action and object to predict the sensory consequence of this interaction is a top&#x02013;down, cognitive controlled processing. The current findings are different from previous studies using first-person perspective visual stimuli which support that the processing of hand&#x02013;object interaction is rapid and automatic.</p></abstract><kwd-group><kwd>event-related potentials (ERPs)</kwd><kwd>action perception</kwd><kwd>pain</kwd><kwd>sensory expectation</kwd></kwd-group><funding-group><award-group><funding-source id="cn001">National Natural Science Foundation of China<named-content content-type="fundref-id">10.13039/501100001809</named-content></funding-source><award-id rid="cn001">31500877, 31530031, 31300846, 31300847</award-id></award-group></funding-group><counts><fig-count count="3"/><table-count count="1"/><equation-count count="0"/><ref-count count="31"/><page-count count="8"/><word-count count="0"/></counts></article-meta></front><body><sec><title>Introduction</title><p>In our daily life, we constantly witness others handling objects. As social animals, we can derive the information about objects, contexts, and even mental and emotional states of other people from observing these hand&#x02013;object interactions and predict the sensations that are associated with these actions (<xref rid="B26" ref-type="bibr">Morrison et al., 2013</xref>). For example, when we saw a person reaching his hand to grasp the sharp end of a knife, we know he would feel pain before we actually saw his hand touched the knife.</p><p>Knowing others in pain requires the ability of empathy. Neuroimaging evidence suggests that there are two components of empathy subserved by distinct brain networks (<xref rid="B8" ref-type="bibr">Decety and Lamm, 2006</xref>). Specifically, the affective component of empathy has been framed as reflecting rapid bottom&#x02013;up activations of the limbic system (<xref rid="B19" ref-type="bibr">Keysers et al., 2010</xref>, <xref rid="B20" ref-type="bibr">2014</xref>; <xref rid="B29" ref-type="bibr">Rizzolatti and Sinigaglia, 2010</xref>; <xref rid="B21" ref-type="bibr">Lamm et al., 2011</xref>). The cognitive component of empathy, on the other hand, has been shown to be influenced by higher-level, top&#x02013;down, signals originating in prefrontal cortical circuitries (<xref rid="B8" ref-type="bibr">Decety and Lamm, 2006</xref>; <xref rid="B19" ref-type="bibr">Keysers et al., 2010</xref>). Previous event-related potential (ERP) studies support this two-component model by revealing that the empathy for pain involves two key processes: an early, automatic (bottom&#x02013;up) process related to perception&#x02013;action coupling and a later, cognitive controlled (top&#x02013;down) process (<xref rid="B13" ref-type="bibr">Fan and Han, 2008</xref>). ERP research has found that both early (N1 and N2) and later [P3/late positive potential (LPP)] ERP components are sensitive to the comparison of observing others receiving painful stimuli to non-painful stimuli (<xref rid="B13" ref-type="bibr">Fan and Han, 2008</xref>; <xref rid="B15" ref-type="bibr">Han et al., 2008</xref>, <xref rid="B16" ref-type="bibr">2009</xref>; <xref rid="B23" ref-type="bibr">Meng et al., 2013</xref>).</p><p>Most visual stimuli used in studies of empathy for pain were static pictures showing the painful and non-painful consequences of other&#x02019;s actions (<xref rid="B13" ref-type="bibr">Fan and Han, 2008</xref>; <xref rid="B15" ref-type="bibr">Han et al., 2008</xref>, <xref rid="B16" ref-type="bibr">2009</xref>; <xref rid="B23" ref-type="bibr">Meng et al., 2013</xref>). It was found that painful and non-painful stimuli can be distinguished on the early ERP component N1 (peaked at &#x0223c;110 ms; <xref rid="B13" ref-type="bibr">Fan and Han, 2008</xref>). These studies suggested that the human brain can rapid and automatically discriminate whether another person was in pain or not. Some other studies using video-clips showing the dynamic process of a needle stabbing another person&#x02019;s hand also found that the observation of the video automatically induces the covert simulation in the onlooker&#x02019;s corticospinal system. These findings also support that the encoding of other&#x02019;s pain is an automatic but not a top&#x02013;down controlled process (<xref rid="B3" ref-type="bibr">Avenanti et al., 2005</xref>, <xref rid="B4" ref-type="bibr">2009</xref>).</p><p>Regarding that, the next question is: how does the human brain make this prediction? Based on the &#x0201c;sensory expectation&#x0201d; theory, observing other&#x02019;s hand&#x02013;object interactions may involve both action representations and an &#x0201c;expectation&#x0201d; of how the object&#x02019;s properties would affect the sensory surface of the acting person&#x02019;s hand (<xref rid="B12" ref-type="bibr">Eickhoff et al., 2006</xref>; <xref rid="B10" ref-type="bibr">Dijkerman and de Haan, 2007</xref>; <xref rid="B7" ref-type="bibr">Colder, 2015</xref>). The processing of this hand&#x02013;object interaction involves three aspects. First, different action types need to be differentiated. Second, the sensory-tactile qualities of the object need to be coded (e.g., whether the object is noxious or neutral). Third, the two aforementioned aspects of information need to be integrated in a predictive manner to represent the sensory outcome of this hand&#x02013;object interaction (<xref rid="B12" ref-type="bibr">Eickhoff et al., 2006</xref>; <xref rid="B14" ref-type="bibr">Gazzola and Keysers, 2009</xref>). A recent functional magnetic resonance imaging (fMRI) study has provided neural evidence of the existence of these three aspects. In that study, participants observed other&#x02019;s hands grasping or withdrawing from either noxious or neutral objects, and the results showed that distinct sensorimotor subregions represented preferential responses to different aspects of the stimuli: object noxiousness (noxious vs. neutral), action type (grasp vs. withdrawal), and painful action outcomes (painful grasps vs. all other conditions). More specifically, separate somatosensory/inferior partial lobule (IPL) subregions responded more strongly when the observed action targeted at a noxious object compared with a neutral object, regardless of the action type. Other subregions responded more strongly to observed grasps than to observed withdrawals, regardless of whether the object was noxious or not. Finally, a region in the somatosensory cortices was found to be activated only in the condition in which the hand&#x02013;object interaction would cause a painful consequence (<xref rid="B26" ref-type="bibr">Morrison et al., 2013</xref>).</p><p>To our knowledge, the temporal aspects of this processing have not been explored yet. In the present study, we aimed to explore the temporal dynamics of this processing. We also noticed that all the visual stimuli (pictures or video-clips) used in previous experiments only showed views from the first-person perspective. Based on our daily experience, we know that human beings can also predict other&#x02019;s pain precisely when the visual input was from the third-person perspective. Therefore in the current study, we also want to explore how the brain predicts other&#x02019;s pain from a third-person perspective. The participants were presented with pictures that showed a hand grasping or withdrawing from a noxious or neutral object and were asked to judge whether the observed hand&#x02013;object interaction could cause a painful consequence. We then compared ERPs when the participants observed different kinds of pictures.</p><p>The present study is exploratory, and previous literature on the same topic is limited. In our opinion, when observing pictures showing an ongoing hand&#x02013;object interaction from the third-person perspective, the observers would not be able to distinguish painful consequence from non-painful consequence as quickly as in the first-person perspective condition. This is because the third-person perspective may prevent the participants from directly &#x0201c;embedded&#x0201d; other&#x02019;s emotional feelings. Instead, they may need to encode the information of action types and object properties first, and then to integrate different information to finish the prediction. Therefore, we expected to find the main effect of action and/or object in the earlier stage of stimulus processing (such as the N1, P1, and N2 components), and find the interaction of action &#x000d7; object in the later stage (such as the P3 and LPP components). Meanwhile, whether the prediction of other&#x02019;s pain is conducted in an automatic or a top&#x02013;down cognitive-controlled style would depend on at which temporal stage we can find the significant interaction of action &#x000d7; object. Specifically, if the interaction of action &#x000d7; object was found on later ERP components (e.g., P3 and/or LPP), these results would suggest that the integration is a top&#x02013;down process. Otherwise, it is possible that the integration process happened automatically.</p></sec><sec sec-type="materials|methods" id="s1"><title>Materials and Methods</title><sec><title>Participants</title><p>Eighteen right-handed participants (10 male; 21.25 &#x000b1; 0.73 years [mean &#x000b1;<italic>SE</italic>]) with no history of neurological disorders, brain injury, or developmental disabilities participated in the study. All of them have normal or corrected-to-normal vision. The study was approved by the Medical Ethical Committee of Shenzhen University Medical School. All of the participants provided written informed consent. They received monetary compensation for participation.</p></sec><sec><title>Stimuli</title><p>The stimuli consisted of 32 color pictures that showed a human hand grasping or withdrawing from a noxious or a neutral object. All of the objects appeared in the pictures can be grasped with a power grip. Two categories of objects were presented (noxious and neutral), with eight objects in each category (<bold>Supplementary Table <xref ref-type="supplementary-material" rid="SM2">S1</xref></bold>).</p><p>The neutral and noxious objects in the pictures have been used in previous studies. The level of dangerousness of the objects has been evaluated in the original studies and the categorization of the noxious and neutral objects have been demonstrated to be valid (<xref rid="B1" ref-type="bibr">Anelli et al., 2012</xref>, <xref rid="B2" ref-type="bibr">2013</xref>). There were four kinds of pictures: grasping a noxious object, grasping a neutral object, withdrawing from a noxious object, and withdrawing from a neutral object. Each kind consisted of eight different pictures (<bold>Figure <xref ref-type="fig" rid="F1">1A</xref></bold> shows an example of each category). All of the pictures were identical with regard to size, background, contrast, brightness, and other physical properties. All of them were presented on a black background (3.0&#x000b0; &#x000d7; 3.5&#x000b0; of visual angle).</p><fig id="F1" position="float"><label>FIGURE 1</label><caption><p><bold>(A)</bold> Examples of pictures in each condition <bold>(B)</bold> an example from one trial.</p></caption><graphic xlink:href="fpsyg-07-01847-g001"/></fig></sec><sec><title>Experimental Procedures</title><p>The stimulus display and behavioral data acquisition were performed using E-Prime 2.0 software (Psychology Software Tools). During the task, the participants sat comfortably in an electrically shielded room approximately 90 cm from a 15-inch color monitor. The participants were informed that they need to do a &#x0201c;predict pain&#x0201d; task. They would be presented with pictures showing a human hand grasping or withdrawing from an object. They had a maximum of 2.5 s to judge whether this hand&#x02013;object interaction would cause a painful sensation in the executor. Each trial began with a fixation cross presented in the center of the screen for 500 ms followed by 400&#x02013;700 ms interval. Then the picture appeared for a maximum of 2500 ms and disappeared until a response was given. If the participant believed the hand&#x02013;object interaction shown in the picture would cause pain, then they press the &#x0201c;F&#x0201d; on the keyboard; otherwise, they press the &#x0201c;J&#x0201d;. The keyboard letter assignments were counterbalanced. There was a 1200&#x02013;1800 ms inter-trial interval (<bold>Figure <xref ref-type="fig" rid="F1">1B</xref></bold>). A total of 224 trials were conducted, evenly separated into two blocks. Each of the 32 pictures was repeated seven times.</p><p>The study utilized a 2 &#x000d7; 2 within-subjects design. The first factor was the object (noxious or neutral). The second factor was the action (grasp or withdrawal). The experiment had a total of four conditions: Grasp-Noxious (GNo), Grasp-Neutral (GNe), Withdrawal-Noxious (WNo), and Withdrawal-Neutral (WNe). After the participants completed the task, they were asked to rate the degree of painfulness for each picture on a 7-point Likert scale (1 = not painful at all and 7 = extremely painful).</p></sec><sec><title>Electroencephalographic (EEG) Acquisition and Analysis</title><p>Electroencephalographic data were recorded from a 64-electrode scalp cap using the 10&#x02013;20 system (Brain Products, Munich, Germany) with reference electrodes on the left and right mastoids. The vertical electrooculogram (EOG) was recorded with placed above and below the left eye. EEG and EOG activity was amplified at 0.01&#x02013;100 Hz band-pass and sampled at 500 Hz. All of the electrode impedances were maintained below 5 k&#x003a9;.</p><p>The EEG data were pre-processed and analyzed using Matlab R2011b software (MathWorks) and the EEGLAB toolbox (<xref rid="B9" ref-type="bibr">Delorme and Makeig, 2004</xref>). The EEG data for each electrode were down-sampled to 250 Hz and re-referenced to the grand averages. The signal was then passed through a 0.01- to 30-Hz band-pass filter. Time windows of 200 ms before and 700 ms after the onset of the picture were segmented. EOG artifacts were corrected using an independent component analysis (ICA) (<xref rid="B18" ref-type="bibr">Jung et al., 2001</xref>) (<bold>Supplementary Figure <xref ref-type="supplementary-material" rid="SM1">S1</xref></bold>). Epochs with amplitudes that exceeded &#x000b1;50 &#x003bc;V at any electrode were excluded from the average (5.6 &#x000b1; 0.6% trials were excluded).</p></sec><sec><title>Data Measurement and Analysis</title><p>We calculated the accuracy (ACC) and reaction times (RTs) as the behavioral indices of the participants in this &#x0201c;predict pain&#x0201d; task. A within-subjects repeated-measures analysis of variance (ANOVA) was performed, with action and object as two within-subjects factors. The participants were asked to rate &#x0201c;How painful would the hand&#x02013;object interaction be?&#x0201d; for all pictures. The ratings for the four conditions were analyzed using the same within-subject repeated-measures ANOVA. All of the analyses were performed using SPSS 22 software.</p><p>The follow-up analyses focused on the ERPs elicited by observing pictures. The averaged epoch was 900 ms, including a 200 ms pre-stimulus baseline. Since the current study is exploratory, the statistical analysis was conducted at electrodes selected from five regions that covered the whole scalp: frontal (Fz, FCz, F3&#x02013;F4, and FC3&#x02013;FC4), central (Cz, CPz, C3&#x02013;C4, and CP3&#x02013;CP4), parietal (Pz and P3&#x02013;P4), temporal (T7&#x02013;T8, TP7&#x02013;TP8, and P7&#x02013;P8), and occipito-temporal (POz, Oz, PO3&#x02013;PO4, and PO7&#x02013;PO8) regions (<xref rid="B13" ref-type="bibr">Fan and Han, 2008</xref>). Based on the literature, observing affective picture can trigger ERP components from short (N1, P2, and N2) to long (P3 and LPP) latencies (for a review: <xref rid="B30" ref-type="bibr">Schupp et al., 2000</xref>). In the current dataset, the mean ERP waves from each regions and the topographical distributions were inspected to determine the characteristics of aforementioned components, specifically, the N1 (90&#x02013;140 ms), P2 (160&#x02013;200ms), N2 (240&#x02013;300 ms), P3 (250&#x02013;350 ms), and the LPP (400&#x02013;550 ms) components. The peak amplitudes for each time window from all of the five regions were subjected to a three-way repeated-measures ANOVA with object (noxious and neutral), action (grasp and withdrawal), and region (frontal, central, parietal, temporal, and occipital&#x02013;temporal) as within-subjects factors. The degrees of freedom for <italic>F</italic>-ratios were corrected according to the Greenhouse&#x02013;Geisser method. Differences were considered statistically significant at <italic>p</italic> &#x0003c; 0.05. For the sake of brevity, only significant effects are reported hereinafter.</p></sec></sec><sec><title>Results</title><sec><title>Behavioral Data</title><p>For the ACC, we found a significant main effect of action (<italic>F</italic><sub>1,17</sub> = 33.551, <italic>p</italic> &#x0003c; 0.001, and <inline-formula><mml:math id="M1"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.664); when the action was &#x0201c;withdrawal&#x0201d;, the ACC of judgment was significantly higher than when the action was &#x0201c;grasp&#x0201d;(grasp: 93.2 &#x000b1; 0.7% and withdrawal: 97.3 &#x000b1; 0.6%). We also found a significant main effect of object (<italic>F</italic><sub>1,17</sub> = 5.369, <italic>p</italic> = 0.033, and <inline-formula><mml:math id="M2"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.240); when the object was neutral, the ACC was significantly higher than when the object was noxious (neutral: 96.5 &#x000b1; 0.6% and noxious: 94.1 &#x000b1; 0.9%). The interaction of action &#x000d7; object was not significance (<italic>F</italic><sub>1,17</sub> = 0.235, <italic>p</italic> = 0.634, and <inline-formula><mml:math id="M3"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.014).</p><p>For the RT, a significant main effect of action (<italic>F</italic><sub>1,17</sub> = 35.953, <italic>p</italic> &#x0003c; 0.001, and <inline-formula><mml:math id="M4"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.679) was observed: when the action was &#x0201c;withdrawal&#x0201d;, RT was significantly shorter than when the action was &#x0201c;grasp&#x0201d; (grasp: 778.690 &#x000b1; 27.873 ms; withdrawal: 669.258 &#x000b1; 29.518 ms). A significant main effect of object (<italic>F</italic><sub>1,17</sub> = 5.041, <italic>p</italic> = 0.038, and <inline-formula><mml:math id="M5"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.229) was also found to be significant: when the object was neutral, the RT was significantly shorter than when the object was noxious (neutral: 708.237 &#x000b1; 28.771 ms and noxious: 739.711 &#x000b1; 27.426 ms). The interaction of action &#x000d7; object was close to significant (<italic>F</italic><sub>1,17</sub> = 3.754, <italic>p</italic> = 0.069, and <inline-formula><mml:math id="M6"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.181).</p><p>For the subjective rating of painfulness of the hand&#x02013;object interaction, we found a significant main effect of action (<italic>F</italic><sub>1,17</sub> = 235.003, <italic>p</italic> &#x0003c; 0.001, and <inline-formula><mml:math id="M7"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.933); when the action was withdrawal, the rating was significantly lower than when the action was grasp (grasp: 3.855 &#x000b1; 0.123 and withdrawal: 1.306 &#x000b1; 0.084) as well as a significant main effect of object (<italic>F</italic><sub>1,17</sub> = 289.346, <italic>p</italic> &#x0003c; 0.001, and <inline-formula><mml:math id="M8"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.945); when the object was neutral, the rating was significantly lower than when the object was noxious (neutral: 1.378 &#x000b1; 0.066 and noxious: 3.783 &#x000b1; 0.118).</p></sec><sec><title>ERP Data</title><p><italic>N1</italic>. The main effect of region was significant (<italic>F</italic><sub>4,68</sub> = 7.788, <italic>p</italic> = 0.003, and <inline-formula><mml:math id="M9"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.314) indicating that N1 reached its maximum in the frontal region (frontal: -2.533 &#x000b1; 0.266 &#x003bc;V, central: -1.481 &#x000b1; 0.190 &#x003bc;V, parietal: -0.475 &#x000b1; 0.398 &#x003bc;V, temporal: -0.363 &#x000b1; 0.271 &#x003bc;V, and occipito-temporal: -0.067 &#x000b1; 0.604 &#x003bc;V). The main effect of action was significant (<italic>F</italic><sub>1,17</sub> = 15.566, <italic>p</italic> = 0.001, and <inline-formula><mml:math id="M10"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.478): the amplitude was more negative when the action was &#x0201c;grasp&#x0201d; than when the action was &#x0201c;withdrawal&#x0201d; (grasp: -1.156 &#x000b1; 0.202 &#x003bc;V and withdrawal: -0.811 &#x000b1; 0.181 &#x003bc;V).</p><p>The interaction of action &#x000d7; region was significant (<italic>F</italic><sub>4,68</sub> = 10.028, <italic>p</italic> &#x0003c; 0.001, and <inline-formula><mml:math id="M11"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.371). Pairwise comparisons revealed that in the frontal region, where the N1 reached its maximum, withdrawal elicited a significantly larger negativity than grasp (grasp: -2.249 &#x000b1; 0.277 &#x003bc;V; -withdrawal: -2.818 &#x000b1; 0.277 &#x003bc;V, <italic>p</italic> = 0.002; <bold>Figures <xref ref-type="fig" rid="F2">2A,B</xref></bold>).</p><fig id="F2" position="float"><label>FIGURE 2</label><caption><p><bold>(A)</bold> Grand average of N1 and N2 components at Fz site in the four conditions; <bold>(B)</bold> voltage scalp maps for N1 and N2 in four conditions</p></caption><graphic xlink:href="fpsyg-07-01847-g002"/></fig><p><italic>P2</italic>. A significant main effect of region was observed (<italic>F</italic><sub>4,68</sub> = 7.986, <italic>p</italic> = 0.006, and <inline-formula><mml:math id="M12"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.320), indicating the P2 reached its maximum in the occipito-temporal region (frontal: 0.911 &#x000b1; 0.543 &#x003bc;V, central: -0.177 &#x000b1; 0.230 &#x003bc;V, parietal: 1.381 &#x000b1; 0.457 &#x003bc;V, temporal: 1.042 &#x000b1; 0.365 &#x003bc;V, and occipito-temporal: 3.957 &#x000b1; 0.835 &#x003bc;V).</p><p>The interaction of action &#x000d7; region was significant (<italic>F</italic><sub>4,68</sub> = 6.697, <italic>p</italic> = 0.001, and <inline-formula><mml:math id="M13"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.283). Pairwise comparisons showed that in the occipito-temporal region where the P2 reached its peak, the difference between two actions were insignificant (grasp: 3.898 &#x000b1; 0.803 &#x003bc;V; withdrawal: 4.257 &#x000b1; 0.888 &#x003bc;V, <italic>p</italic> = 0.077). Therefore, we cannot conclude the effect of action was significant in P2.</p><p><italic>N2</italic>. The main effect of region was significant (<italic>F</italic><sub>4,68</sub> = 17.027, <italic>p</italic> &#x0003c; 0.001, and <inline-formula><mml:math id="M14"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.500), indicating that the N2 mainly distributed in the frontal region (frontal: -2.522 &#x000b1; 0.342 &#x003bc;V, central: -0.056 &#x000b1; 0.322 &#x003bc;V, parietal: 1.911 &#x000b1; 0.376 &#x003bc;V, temporal: -0.218 &#x000b1; 0.369 &#x003bc;V, and occipito-temporal: 2.004 &#x000b1; 0.667 &#x003bc;V).</p><p>The interaction of action &#x000d7; region was significant (<italic>F</italic><sub>4,68</sub> = 4.487, <italic>p</italic> = 0.027, and <inline-formula><mml:math id="M15"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.209) and pairwise comparisons revealed that in the frontal region where the N2 reached its peak, grasp elicited a significantly larger negativity than withdrawal (grasp: -2.817 &#x000b1; 0.293 &#x003bc;V; withdrawal: -2.228 &#x000b1; 0.417 &#x003bc;V, <italic>p</italic> = 0.018; <bold>Figures <xref ref-type="fig" rid="F2">2A,B</xref></bold>).</p><p><italic>P3</italic>. A significant main effect of region was observed (<italic>F</italic><sub>4,68</sub> = 26.788, <italic>p</italic> &#x0003c; 0.001, and <inline-formula><mml:math id="M16"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.612), indicating that the amplitude of P3 component was maximum in the parietal region (frontal: -0.530 &#x000b1; 0.366 &#x003bc;V, central: 2.302 &#x000b1; 0.274 &#x003bc;V, parietal: 4.972 &#x000b1; 0.358 &#x003bc;V, temporal: 1.249 &#x000b1; 0.330 &#x003bc;V, and occipito-temporal: 2.341 &#x000b1; 0.661 &#x003bc;V).</p><p>The interaction of action &#x000d7; region was significant (F<sub>4,68</sub> = 9.121, <italic>p</italic> &#x0003c; 0.001, and<inline-formula><mml:math id="M17"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.349). Pairwise comparisons indicated that in the parietal region, grasp elicited a significantly larger negativity than withdrawal (grasp: 5.476 &#x000b1; 0.374 &#x003bc;V; withdrawal: 4.469 &#x000b1; 0.401 &#x003bc;V, <italic>p</italic> = 0.004). A three way interaction of action &#x000d7; object &#x000d7; region was significant (<italic>F</italic><sub>4,68</sub> = 3.518, <italic>p</italic> = 0.033, and <inline-formula><mml:math id="M18"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.171). Pairwise comparisons revealed that in the parietal region, only when the object was noxious, the action &#x0201c;grasp&#x0201d; elicited a significantly larger negativity than the action &#x0201c;withdrawal&#x0201d; (grasp: 5.529 &#x000b1; 0.407 &#x003bc;V; withdrawal: 4.176 &#x000b1; 0.391 &#x003bc;V, <italic>p</italic> = 0.001); when the object was neutral, the difference between two action types was insignificant (grasp: 5.422 &#x000b1; 0.365 &#x003bc;V; withdrawal: 4.762 &#x000b1; 0.439 &#x003bc;V, <italic>p</italic> = 0.074; <bold>Figures <xref ref-type="fig" rid="F3">3A&#x02013;C</xref></bold>).</p><fig id="F3" position="float"><label>FIGURE 3</label><caption><p><bold>(A)</bold> Grand average and voltage scalp map of P3 and late positive potential (LPP) component at Pz; <bold>(B)</bold> voltage scalp map of P3 and LPP components <bold>(C)</bold> the interaction of action &#x000d7; object of P3 and LPP components in the parietal region.</p></caption><graphic xlink:href="fpsyg-07-01847-g003"/></fig><p><italic>LPP</italic>. The main effect of region was significant (<italic>F</italic><sub>4,68</sub> = 21.910, <italic>p</italic> &#x0003c; 0.001, and <inline-formula><mml:math id="M19"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.563), indicating the amplitude of the LPP was maximum in the parietal region (frontal: -0.199 &#x000b1; 0.349 &#x003bc;V, central: 2.759 &#x000b1; 0.250 &#x003bc;V, parietal: 4.609 &#x000b1; 0.413 &#x003bc;V, temporal: 1.294 &#x000b1; 0.341 &#x003bc;V, and occipito-temporal: 3.545 &#x000b1; 0.621 &#x003bc;V). The main effect of action was significant (<italic>F</italic><sub>1,17</sub> = 5.290, <italic>p</italic> = 0.034, and <inline-formula><mml:math id="M20"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.237): the amplitude was larger when the action was &#x0201c;grasp&#x0201d; than when the action was &#x0201c;withdrawal&#x0201d; (grasp: 2.487 &#x000b1; 0.205 &#x003bc;V and withdrawal: 2.316 &#x000b1; 0.206 &#x003bc;V).</p><p>A three way interaction of action &#x000d7; object &#x000d7; region was significant (<italic>F</italic><sub>4,68</sub> = 5.096, <italic>p</italic> = 0.008, and <inline-formula><mml:math id="M21"><mml:msubsup><mml:mi mathvariant="normal" mathcolor="black">&#x003b7;</mml:mi><mml:mi mathvariant="normal" mathcolor="black">p</mml:mi><mml:mn mathvariant="normal" mathcolor="black">2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.231). Pairwise comparisons revealed that in the parietal region, only when the object was noxious, the action &#x0201c;grasp&#x0201d; elicited a significantly larger LPP amplitude than the action &#x0201c;withdrawal&#x0201d; (grasp: 5.188 &#x000b1; 0.447 &#x003bc;V; withdrawal: 4.029 &#x000b1; 0.404 &#x003bc;V, <italic>p</italic> &#x0003c; 0.001); when the object was neutral, the difference was insignificant (grasp: 4.618 &#x000b1; 0.390 &#x003bc;V; withdrawal: 4.600 &#x000b1; 0.517 &#x003bc;V, <italic>p</italic> = 0.953; <bold>Figures <xref ref-type="fig" rid="F3">3A&#x02013;C</xref></bold>).</p></sec></sec><sec><title>Discussion</title><p>The present study aimed to explore the temporal dynamics of predicting the expected sensory consequences of the observed hand&#x02013;object interactions of others. Participants were asked to watch others&#x02019; hands either grasping or withdrawing from objects that were either noxious or neutral and judged whether this interaction would cause painful consequences. The application of the ERPs allowed us to explore in which temporal stage of stimulus processing, the object (noxious or neutral), action (grasp or withdrawal), and their integration (whether this hand&#x02013;object interaction would cause pain) were evaluated.</p><p>Regarding the behavioral results, it was suggesting that when the action type or the property of object in the hand&#x02013;object interaction has the potential to cause harm, the prediction task became more difficult for the participants. It is worth noticing that we do not observe any significant action &#x000d7; object interaction in behavioral data. Though we do found the interaction of action &#x000d7; object was close to significance in the RTs (<italic>p</italic> = 0.069). We can tell from the data that when the action was withdrawal, the difference between neutral and noxious object was relatively larger than when the action was grasp (<bold>Table <xref ref-type="table" rid="T1">1</xref></bold>). The insignificance may due to the limited sensitivity of behavioral measurements.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p>The first four columns present the two factors in the design and coding of each condition.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><td valign="top" align="left" rowspan="1" colspan="1"/><th valign="top" align="center" colspan="2" rowspan="1">Action</th></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="left" colspan="2" rowspan="1"><hr/></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><th valign="top" align="center" rowspan="1" colspan="1">Grasp</th><th valign="top" align="center" rowspan="1" colspan="1">Withdrawal</th></tr></thead><tbody><tr><td valign="top" align="left" colspan="3" rowspan="1"><bold>ACCs (%)</bold></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Neutral object</td><td valign="top" align="center" rowspan="1" colspan="1">93.73 &#x000b1; 5.10</td><td valign="top" align="center" rowspan="1" colspan="1">99.19 &#x000b1; 1.76</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Noxious object</td><td valign="top" align="center" rowspan="1" colspan="1">92.74 &#x000b1; 4.70</td><td valign="top" align="center" rowspan="1" colspan="1">95.50 &#x000b1; 4.14</td></tr><tr><td valign="top" align="left" colspan="3" rowspan="1"><bold>RTs (ms)</bold></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Neutral object</td><td valign="top" align="center" rowspan="1" colspan="1">783.64 &#x000b1; 126.62</td><td valign="top" align="center" rowspan="1" colspan="1">632.93 &#x000b1; 126.02</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Noxious object</td><td valign="top" align="center" rowspan="1" colspan="1">773.74 &#x000b1; 125.16</td><td valign="top" align="center" rowspan="1" colspan="1">705.68 &#x000b1; 131.55</td></tr><tr><td valign="top" align="left" colspan="3" rowspan="1"><bold>Ratings</bold></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Neutral object</td><td valign="top" align="center" rowspan="1" colspan="1">1.713 &#x000b1; 0.56</td><td valign="top" align="center" rowspan="1" colspan="1">1.042 &#x000b1; 0.11</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Noxious object</td><td valign="top" align="center" rowspan="1" colspan="1">5.996 &#x000b1; 0.73</td><td valign="top" align="center" rowspan="1" colspan="1">1.571 &#x000b1; 0.67</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/></tr></tbody></table><table-wrap-foot><attrib><italic>The latter three columns present accuracy (ACC) and reaction times (RTs) for each condition. The last column is the subjective rating of the degree of painfulness for each kind of picture stimulus (mean &#x000b1;<italic>SD</italic>)</italic></attrib></table-wrap-foot></table-wrap><p>With regards to the ERP data, on the N1 component (peaked at 110 ms) we found that the action &#x0201c;withdrawal&#x0201d; elicited significantly larger negative amplitude than the action &#x0201c;grasp&#x0201d;. This effect indicates that object-oriented actions (grasp) and non-object-oriented actions (withdrawal) are differentiated in the early visual processing stage. The N1 component may reflect the activity of a neural population that is involved in the early integration of agent form (e.g, a human being or a robot) and motion type (<xref rid="B5" ref-type="bibr">Baccus et al., 2009</xref>). Previous studies have found that neural responses to the onset of movements of the mouth and eyes could be observed within 200 ms after motion onset (<xref rid="B28" ref-type="bibr">Puce et al., 2000</xref>). Similar results were found for the observation of whole-body actions (e.g., walking) of others (<xref rid="B31" ref-type="bibr">Wheaton et al., 2001</xref>). This effect may suggest that the human visual system is very efficient in detecting human actions in a visual scene (<xref rid="B17" ref-type="bibr">Jokisch et al., 2005</xref>).</p><p>The effect of action was also observed on the component N2 (peaked 280 ms) where the &#x0201c;grasp&#x0201d; elicited significantly larger amplitudes than the &#x0201c;withdrawal&#x0201d;. The literature suggests that high arousing (regardless of its emotional valence) stimuli elicit a more pronounced N2 than low arousing (neutral) stimuli (<xref rid="B27" ref-type="bibr">Olofsson and Polich, 2007</xref>), which may indexes an evolutionarily adaptive attentional bias such that the evaluation of image features is inclined to affectively arousing stimuli for further processing (<xref rid="B30" ref-type="bibr">Schupp et al., 2000</xref>; <xref rid="B11" ref-type="bibr">Dolcos and Cabeza, 2002</xref>). In the current study, &#x0201c;grasp&#x0201d; would be more arousing than &#x0201c;withdrawal&#x0201d; because the former one is an object-oriented action which needs further processing.</p><p>An important human ability is responding properly to various objects in the environment. Previous behavioral and psychophysiological studies suggest that people are sensitive to the differences between noxious and neutral objects (<xref rid="B1" ref-type="bibr">Anelli et al., 2012</xref>, <xref rid="B2" ref-type="bibr">2013</xref>). In the present data, we do not observe any effect of object in the early components such as N1, P2, and N2, implying that the early stages of stimulus processing are dominated by the processing of action but not the processing of object. These findings are consistent with a previous study which found significantly larger P3 and LPP when phobic participants were presented with fearful stimuli compared with control participants but no effect in the early ERP components (<xref rid="B24" ref-type="bibr">Miltner et al., 2005</xref>). The literature suggests that objects in the environment are taken into consideration when they potentially offer an opportunity to a subject or might signal a threat, in either case the salience of an object depends upon the motivation of the subjects. The key of human instinct is not to respond to objects per se, but to evaluate alternative actions in a given context (<xref rid="B25" ref-type="bibr">Mirabella, 2014</xref>). Regarding that, the human brain is more likely to process an object when it is the target of somebody. Although no effect was observed in the ERP result to indicate the encoding of object property alone, we should note that being aware of the potentially dangerous object in the circumstance is crucial for surviving. Therefore, when they appear in the sight, they can distract attention from the ongoing task. Regarding the behavioral results, when the object was noxious, the task was performed more slowly and with lower ACC comparing to when the object was neutral. The subjective evaluation also reflect the harmfulness of the object, that is, even when the other&#x02019;s hand was withdrawing from the object, the observer still felt danger.</p><p>Finally, to readily predict the sensory consequence of others&#x02019; actions, observers not only need to encode the action type or/and the object property but also need to integrate these two aspects of information to form a prediction. Neuroimaging evidence suggested that different brain regions underlie the encoding of action, the encoding of object, and the integration of the two aspects in the judgments of an action&#x02019;s appropriateness (e.g., whether this hand&#x02013;object interaction would cause a painful consequence) (<xref rid="B26" ref-type="bibr">Morrison et al., 2013</xref>). From the temporal aspect, the current results find significant interactions between action and object in the later components P3 and LPP. For both the P3 and LPP, only when the object was noxious, the action &#x0201c;grasp&#x0201d; elicited significantly larger amplitudes than the action &#x0201c;withdrawal&#x0201d;; but when the object was neutral, the difference between these two types of action was insignificant. These results indicate that in the temporal dimension, the interaction of action and object happens later than the encoding of action and it is a cognitive-controlled process. However, as we know that pain is intimately linked with action systems, so as to allow people to freeze or escape for survival. Regarding that, if pain (and empathy for pain) guides adaptive homeostatic responses, the processing of painful stimuli must be very quick (<xref rid="B22" ref-type="bibr">LeDoux, 2014</xref>). Previous studies also suggest that empathy for pain is an automatic but not a top&#x02013;down process (<xref rid="B3" ref-type="bibr">Avenanti et al., 2005</xref>, <xref rid="B4" ref-type="bibr">2009</xref>). Then why in the present study we find that predicting other&#x02019;s pain is not automatic? We propose that this question could be explained by the perspective of the presentation of the stimuli. The visual stimuli used in previous studies all show views from the first-person perspective while in the current study the pictures show view from the third-person perspective. Compared to the first-person perspective, when viewing the scene from the third-person perspective, it may be more difficult for the observers to directly put themselves into other&#x02019;s shoes. Consistent with this point of view, a recent behavioral study finds that participants detected a tactile object that was delivered to other&#x02019;s hand when the stimuli were presented from a first-person perspective, but not when the stimuli were presented from a third-person perspective. To explain these findings, the authors suggest that to empathize with other&#x02019;s feelings, the sensory consequences of the other&#x02019;s actions need to be represented in the observer&#x02019;s own tactile representation system; in the third-person perspective these effects are restricted by basic components of the others&#x02019; actions, such as the object properties and action types. More high-level components, such as the integration of the object and action for predicting sensory consequences cannot be processed directly but need to be transferred to the first-person perspective first(<xref rid="B6" ref-type="bibr">Bach et al., 2014</xref>). Accordingly, we suggest that the current study offers psychophysical evidence for this assumption and proves that when the stimuli are presented from the third-person perspective, the integration of the action and object may involve more top&#x02013;down processing.</p><p>In summary, the present study has found that when predicting the sensory consequence of other&#x02019;s hand&#x02013;object interaction, the action type is encoded in the early stages of stimulus processing (manifested on the N1 and N2 components). Subsequently, the interaction of action type and object property happens mainly in the later stage of stimulus processing (manifested on the P3 and LPP). The current results also suggest that when observing an ongoing hand&#x02013;object interaction from the third-person perspective, the prediction of other&#x02019;s pain is more likely to be a cognitive controlled top&#x02013;down process, rather than an automatic one.</p><sec><title>Limitations</title><p>There are two limitations of the present study which should be noted. First of all, we used static pictures to present a dynamic action. Before the task, we informed the participants that they would observe pictures showing an ongoing action. In addition, the feedback questionnaire has proved that the participants did feel they were observing ongoing hand actions. However, there are still obvious difference between static pictures and movies that showing dynamic actions. Future studies should consider using active stimuli to explore this topic. Second, in all the stimuli used in the current study, all the hands were placed in the top half of the picture and objects were placed in the bottom half. Consequently, the participants might have paid attention to the hand first. In future studies, we should balance the location of hand and object to avoid this potential confound.</p></sec></sec><sec><title>Author Contributions</title><p>FC contributed in designing the experiment, analyzing the data, and writing the manuscript. RG contributed in analyzing the data and writing the manuscript. XZ contributed in collecting the data and analyzing the data, and YL contributed in writing the manuscript.</p></sec><sec><title>Conflict of Interest Statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></body><back><ack><p>This study was funded by the National Natural Science Foundation of China (no. 31500877, 31530031,31300846, and 31300847) and the National Basic Research Program of China (973 programs) (no. 2014CB744600).</p></ack><sec sec-type="supplementary material"><title>Supplementary Material</title><p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="http://journal.frontiersin.org/article/10.3389/fpsyg.2016.01847/full#supplementary-material">http://journal.frontiersin.org/article/10.3389/fpsyg.2016.01847/full#supplementary-material</ext-link></p><supplementary-material content-type="local-data" id="SM1"><media xlink:href="Image_1.TIF"><caption><p>Click here for additional data file.</p></caption></media><p><bold>FIGURE S1 | (A)</bold> The EEG signal before and after using independent component analysis (ICA) removing the artifact of eye blink and <bold>(B)</bold> the component reflecting the eye blinks identified using ICA (marked with yellow circle).</p></supplementary-material><supplementary-material content-type="local-data" id="S1"><media xlink:href="Image_1.TIF"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="SM2"><media xlink:href="Table_1.DOCX"><caption><p>Click here for additional data file.</p></caption></media><p><bold>TABLE S1 | Two categories of objects: noxious and neutral with eight objects in each category.</bold></p></supplementary-material><supplementary-material content-type="local-data" id="S2"><media xlink:href="Table_1.DOCX"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anelli</surname><given-names>F.</given-names></name><name><surname>Borghi</surname><given-names>A. M.</given-names></name><name><surname>Nicoletti</surname><given-names>R.</given-names></name></person-group> (<year>2012</year>). <article-title>Grasping the pain: motor resonance with dangerous affordances.</article-title>
<source><italic>Conscious. Cogn.</italic></source>
<volume>21</volume>
<fpage>1627</fpage>&#x02013;<lpage>1639</lpage>. <pub-id pub-id-type="doi">10.1016/j.concog.2012.09.001</pub-id><pub-id pub-id-type="pmid">23041720</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anelli</surname><given-names>F.</given-names></name><name><surname>Ranzini</surname><given-names>M.</given-names></name><name><surname>Nicoletti</surname><given-names>R.</given-names></name><name><surname>Borghi</surname><given-names>A. M.</given-names></name></person-group> (<year>2013</year>). <article-title>Perceiving object dangerousness: an escape from pain?</article-title>
<source><italic>Exp. Brain Res.</italic></source>
<volume>228</volume>
<fpage>457</fpage>&#x02013;<lpage>466</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-013-3577-2</pub-id><pub-id pub-id-type="pmid">23743714</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avenanti</surname><given-names>A.</given-names></name><name><surname>Bueti</surname><given-names>D.</given-names></name><name><surname>Galati</surname><given-names>G.</given-names></name><name><surname>Aglioti</surname><given-names>S. M.</given-names></name></person-group> (<year>2005</year>). <article-title>Transcranial magnetic stimulation highlights the sensorimotor side of empathy for pain.</article-title>
<source><italic>Nat. Neurosci.</italic></source>
<volume>8</volume>
<fpage>955</fpage>&#x02013;<lpage>960</lpage>. <pub-id pub-id-type="doi">10.1038/nn1481</pub-id><pub-id pub-id-type="pmid">15937484</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avenanti</surname><given-names>A.</given-names></name><name><surname>Minio-Paluello</surname><given-names>I.</given-names></name><name><surname>Sforza</surname><given-names>A.</given-names></name><name><surname>Aglioti</surname><given-names>S. M.</given-names></name></person-group> (<year>2009</year>). <article-title>Freezing or escaping? Opposite modulations of empathic reactivity to the pain of others.</article-title>
<source><italic>Cortex</italic></source>
<volume>45</volume>
<fpage>1072</fpage>&#x02013;<lpage>1077</lpage>. <pub-id pub-id-type="doi">10.1016/j.cortex.2008.10.004</pub-id><pub-id pub-id-type="pmid">19100972</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baccus</surname><given-names>W.</given-names></name><name><surname>Mozgova</surname><given-names>O.</given-names></name><name><surname>Thompson</surname><given-names>J. C.</given-names></name></person-group> (<year>2009</year>). <article-title>Early integration of form and motion in the neural response to biological motion.</article-title>
<source><italic>Neuroreport</italic></source>
<volume>20</volume>
<fpage>1334</fpage>&#x02013;<lpage>1338</lpage>. <pub-id pub-id-type="doi">10.1097/WNR.0b013e328330a867</pub-id><pub-id pub-id-type="pmid">19687766</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bach</surname><given-names>P.</given-names></name><name><surname>Fenton-Adams</surname><given-names>W.</given-names></name><name><surname>Tipper</surname><given-names>S. P.</given-names></name></person-group> (<year>2014</year>). <article-title>Can&#x02019;t touch this: the first-person perspective provides privileged access to predictions of sensory action outcomes.</article-title>
<source><italic>J. Exp. Psychol. Hum. Percept. Perform.</italic></source>
<volume>40</volume>
<fpage>457</fpage>&#x02013;<lpage>464</lpage>. <pub-id pub-id-type="doi">10.1037/a0035348</pub-id><pub-id pub-id-type="pmid">24708424</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colder</surname><given-names>B.</given-names></name></person-group> (<year>2015</year>). <article-title>The basal ganglia select the expected sensory input used for predictive coding.</article-title>
<source><italic>Front. Comput. Neurosci.</italic></source>
<volume>9</volume>:<issue>119</issue>
<pub-id pub-id-type="doi">10.3389/fncom.2015.00119</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Decety</surname><given-names>J.</given-names></name><name><surname>Lamm</surname><given-names>C.</given-names></name></person-group> (<year>2006</year>). <article-title>Human empathy through the lens of social neuroscience.</article-title>
<source><italic>ScientificWorldJournal</italic></source>
<volume>6</volume>
<fpage>1146</fpage>&#x02013;<lpage>1163</lpage>. <pub-id pub-id-type="doi">10.1100/tsw.2006.221</pub-id><pub-id pub-id-type="pmid">16998603</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delorme</surname><given-names>A.</given-names></name><name><surname>Makeig</surname><given-names>S.</given-names></name></person-group> (<year>2004</year>). <article-title>EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis.</article-title>
<source><italic>J. Neurosci. Methods</italic></source>
<volume>134</volume>
<fpage>9</fpage>&#x02013;<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id><pub-id pub-id-type="pmid">15102499</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkerman</surname><given-names>H. C.</given-names></name><name><surname>de Haan</surname><given-names>E. H.</given-names></name></person-group> (<year>2007</year>). <article-title>Somatosensory processes subserving perception and action.</article-title>
<source><italic>Behav. Brain Sci.</italic></source>
<volume>30</volume>
<fpage>189</fpage>&#x02013;<lpage>201</lpage>. <pub-id pub-id-type="doi">10.1017/S0140525X07001392</pub-id><pub-id pub-id-type="pmid">17705910</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dolcos</surname><given-names>F.</given-names></name><name><surname>Cabeza</surname><given-names>R.</given-names></name></person-group> (<year>2002</year>). <article-title>Event-related potentials of emotional memory: encoding pleasant, unpleasant, and neutral pictures.</article-title>
<source><italic>Cogn. Affect. Behav. Neurosci.</italic></source>
<volume>2</volume>
<fpage>252</fpage>&#x02013;<lpage>263</lpage>. <pub-id pub-id-type="doi">10.3758/CABN.2.3.252</pub-id><pub-id pub-id-type="pmid">12775189</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eickhoff</surname><given-names>S. B.</given-names></name><name><surname>Amunts</surname><given-names>K.</given-names></name><name><surname>Mohlberg</surname><given-names>H.</given-names></name><name><surname>Zilles</surname><given-names>K.</given-names></name></person-group> (<year>2006</year>). <article-title>The human parietal operculum. II. Stereotaxic maps and correlation with functional imaging results.</article-title>
<source><italic>Cereb. Cortex</italic></source>
<volume>16</volume>
<fpage>268</fpage>&#x02013;<lpage>279</lpage>.<pub-id pub-id-type="pmid">15888606</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>Y.</given-names></name><name><surname>Han</surname><given-names>S.</given-names></name></person-group> (<year>2008</year>). <article-title>Temporal dynamic of neural mechanisms involved in empathy for pain: an event-related brain potential study.</article-title>
<source><italic>Neuropsychologia</italic></source>
<volume>46</volume>
<fpage>160</fpage>&#x02013;<lpage>173</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2007.07.023</pub-id><pub-id pub-id-type="pmid">17825852</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gazzola</surname><given-names>V.</given-names></name><name><surname>Keysers</surname><given-names>C.</given-names></name></person-group> (<year>2009</year>). <article-title>The observation and execution of actions share motor and somatosensory voxels in all tested subjects: single-subject analyses of unsmoothed fMRI data.</article-title>
<source><italic>Cereb. Cortex</italic></source>
<volume>19</volume>
<fpage>1239</fpage>&#x02013;<lpage>1255</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhn181</pub-id><pub-id pub-id-type="pmid">19020203</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>S.</given-names></name><name><surname>Fan</surname><given-names>Y.</given-names></name><name><surname>Mao</surname><given-names>L.</given-names></name></person-group> (<year>2008</year>). <article-title>Gender difference in empathy for pain: an electrophysiological investigation.</article-title>
<source><italic>Brain Res.</italic></source>
<volume>1196</volume>
<fpage>85</fpage>&#x02013;<lpage>93</lpage>. <pub-id pub-id-type="doi">10.1016/j.brainres.2007.12.062</pub-id><pub-id pub-id-type="pmid">18221733</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>S.</given-names></name><name><surname>Fan</surname><given-names>Y.</given-names></name><name><surname>Xu</surname><given-names>X.</given-names></name><name><surname>Qin</surname><given-names>J.</given-names></name><name><surname>Wu</surname><given-names>B.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><etal/></person-group> (<year>2009</year>). <article-title>Empathic neural responses to others&#x02019; pain are modulated by emotional contexts.</article-title>
<source><italic>Hum. Brain Mapp.</italic></source>
<volume>30</volume>
<fpage>3227</fpage>&#x02013;<lpage>3237</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.20742</pub-id><pub-id pub-id-type="pmid">19235883</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jokisch</surname><given-names>D.</given-names></name><name><surname>Daum</surname><given-names>I.</given-names></name><name><surname>Suchan</surname><given-names>B.</given-names></name><name><surname>Troje</surname><given-names>N. F.</given-names></name></person-group> (<year>2005</year>). <article-title>Structural encoding and recognition of biological motion: evidence from event-related potentials and source analysis.</article-title>
<source><italic>Behav. Brain Res.</italic></source>
<volume>157</volume>
<fpage>195</fpage>&#x02013;<lpage>204</lpage>. <pub-id pub-id-type="doi">10.1016/j.bbr.2004.06.025</pub-id><pub-id pub-id-type="pmid">15639170</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jung</surname><given-names>T. P.</given-names></name><name><surname>Makeig</surname><given-names>S.</given-names></name><name><surname>Westerfield</surname><given-names>M.</given-names></name><name><surname>Townsend</surname><given-names>J.</given-names></name><name><surname>Courchesne</surname><given-names>E.</given-names></name><name><surname>Sejnowski</surname><given-names>T. J.</given-names></name></person-group> (<year>2001</year>). <article-title>Analysis and visualization of single-trial event-related potentials.</article-title>
<source><italic>Hum. Brain Mapp.</italic></source>
<volume>14</volume>
<fpage>166</fpage>&#x02013;<lpage>185</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.1050</pub-id><pub-id pub-id-type="pmid">11559961</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keysers</surname><given-names>C.</given-names></name><name><surname>Kaas</surname><given-names>J. H.</given-names></name><name><surname>Gazzola</surname><given-names>V.</given-names></name></person-group> (<year>2010</year>). <article-title>Somatosensation in social perception.</article-title>
<source><italic>Nat. Rev. Neurosci.</italic></source>
<volume>11</volume>
<fpage>417</fpage>&#x02013;<lpage>428</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2833</pub-id><pub-id pub-id-type="pmid">20445542</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keysers</surname><given-names>C.</given-names></name><name><surname>Meffert</surname><given-names>H.</given-names></name><name><surname>Gazzola</surname><given-names>V.</given-names></name></person-group> (<year>2014</year>). <article-title>Reply: spontaneous versus deliberate vicarious representations: different routes to empathy in psychopathy and autism.</article-title>
<source><italic>Brain</italic></source>
<volume>137</volume>:<issue>e273</issue>
<pub-id pub-id-type="doi">10.1093/brain/awt376</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamm</surname><given-names>C.</given-names></name><name><surname>Decety</surname><given-names>J.</given-names></name><name><surname>Singer</surname><given-names>T.</given-names></name></person-group> (<year>2011</year>). <article-title>Meta-analytic evidence for common and distinct neural networks associated with directly experienced pain and empathy for pain.</article-title>
<source><italic>Neuroimage.</italic></source>
<volume>54</volume>
<fpage>2492</fpage>&#x02013;<lpage>2502</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.10.014</pub-id><pub-id pub-id-type="pmid">20946964</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeDoux</surname><given-names>J. E.</given-names></name></person-group> (<year>2014</year>). <article-title>Coming to terms with fear.</article-title>
<source><italic>Proc. Natl. Acad. Sci. U.S.A.</italic></source>
<volume>111</volume>
<fpage>2871</fpage>&#x02013;<lpage>2878</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1400335111</pub-id><pub-id pub-id-type="pmid">24501122</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meng</surname><given-names>J.</given-names></name><name><surname>Jackson</surname><given-names>T.</given-names></name><name><surname>Chen</surname><given-names>H.</given-names></name><name><surname>Hu</surname><given-names>L.</given-names></name><name><surname>Yang</surname><given-names>Z.</given-names></name><name><surname>Su</surname><given-names>Y.</given-names></name><etal/></person-group> (<year>2013</year>). <article-title>Pain perception in the self and observation of others: an ERP investigation.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>72</volume>
<fpage>164</fpage>&#x02013;<lpage>173</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.01.024</pub-id><pub-id pub-id-type="pmid">23376492</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miltner</surname><given-names>W. H.</given-names></name><name><surname>Trippe</surname><given-names>R. H.</given-names></name><name><surname>Krieschel</surname><given-names>S.</given-names></name><name><surname>Gutberlet</surname><given-names>I.</given-names></name><name><surname>Hecht</surname><given-names>H.</given-names></name><name><surname>Weiss</surname><given-names>T.</given-names></name></person-group> (<year>2005</year>). <article-title>Event-related brain potentials and affective responses to threat in spider/snake-phobic and non-phobic subjects.</article-title>
<source><italic>Int. J. Psychophysiol.</italic></source>
<volume>57</volume>
<fpage>43</fpage>&#x02013;<lpage>52</lpage>. <pub-id pub-id-type="doi">10.1016/j.ijpsycho.2005.01.012</pub-id><pub-id pub-id-type="pmid">15896860</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mirabella</surname><given-names>G.</given-names></name></person-group> (<year>2014</year>). <article-title>Should I stay or should I go? Conceptual underpinnings of goal-directed actions.</article-title>
<source><italic>Front. Syst. Neurosci.</italic></source>
<volume>8</volume>:<issue>206</issue>
<pub-id pub-id-type="doi">10.3389/fnsys.2014.00206</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morrison</surname><given-names>I.</given-names></name><name><surname>Tipper</surname><given-names>S. P.</given-names></name><name><surname>Fenton-Adams</surname><given-names>W. L.</given-names></name><name><surname>Bach</surname><given-names>P.</given-names></name></person-group> (<year>2013</year>). <article-title>&#x0201c;Feeling&#x0201d; others&#x02019; painful actions: the sensorimotor integration of pain and action information.</article-title>
<source><italic>Hum. Brain Mapp.</italic></source>
<volume>34</volume>
<fpage>1982</fpage>&#x02013;<lpage>1998</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.22040</pub-id><pub-id pub-id-type="pmid">22451259</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olofsson</surname><given-names>J. K.</given-names></name><name><surname>Polich</surname><given-names>J.</given-names></name></person-group> (<year>2007</year>). <article-title>Affective visual event-related potentials: arousal, repetition, and time-on-task.</article-title>
<source><italic>Biol. Psychol.</italic></source>
<volume>75</volume>
<fpage>101</fpage>&#x02013;<lpage>108</lpage>. <pub-id pub-id-type="doi">10.1016/j.biopsycho.2006.12.006</pub-id><pub-id pub-id-type="pmid">17275979</pub-id></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Puce</surname><given-names>A.</given-names></name><name><surname>Smith</surname><given-names>A.</given-names></name><name><surname>Allison</surname><given-names>T.</given-names></name></person-group> (<year>2000</year>). <article-title>Erps evoked by viewing facial movements.</article-title>
<source><italic>Cogn. Neuropsychol.</italic></source>
<volume>17</volume>
<fpage>221</fpage>&#x02013;<lpage>239</lpage>.<pub-id pub-id-type="pmid">20945181</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzolatti</surname><given-names>G.</given-names></name><name><surname>Sinigaglia</surname><given-names>C.</given-names></name></person-group> (<year>2010</year>). <article-title>The functional role of the parieto-frontal mirror circuit: interpretations and misinterpretations.</article-title>
<source><italic>Nat. Rev. Neurosci.</italic></source>
<volume>11</volume>
<fpage>264</fpage>&#x02013;<lpage>274</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2805</pub-id><pub-id pub-id-type="pmid">20216547</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schupp</surname><given-names>H. T.</given-names></name><name><surname>Cuthbert</surname><given-names>B. N.</given-names></name><name><surname>Bradley</surname><given-names>M. M.</given-names></name><name><surname>Cacioppo</surname><given-names>J. T.</given-names></name><name><surname>Ito</surname><given-names>T.</given-names></name><name><surname>Lang</surname><given-names>P. J.</given-names></name></person-group> (<year>2000</year>). <article-title>Affective picture processing: the late positive potential is modulated by motivational relevance.</article-title>
<source><italic>Psychophysiology</italic></source>
<volume>37</volume>
<fpage>257</fpage>&#x02013;<lpage>261</lpage>. <pub-id pub-id-type="doi">10.1111/1469-8986.3720257</pub-id><pub-id pub-id-type="pmid">10731776</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wheaton</surname><given-names>K. J.</given-names></name><name><surname>Pipingas</surname><given-names>A.</given-names></name><name><surname>Silberstein</surname><given-names>R. B.</given-names></name><name><surname>Puce</surname><given-names>A.</given-names></name></person-group> (<year>2001</year>). <article-title>Human neural responses elicited to observing the actions of others.</article-title>
<source><italic>Vis. Neurosci.</italic></source>
<volume>18</volume>
<fpage>401</fpage>&#x02013;<lpage>406</lpage>. <pub-id pub-id-type="doi">10.1017/S0952523801183069</pub-id><pub-id pub-id-type="pmid">11497416</pub-id></mixed-citation></ref></ref-list></back></article>